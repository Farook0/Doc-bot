{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Pinecone API key\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "index_name = \"mbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone connection\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the index exists; if not, create it\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Adjust based on the embedding model used\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from PDFs\n",
    "def load_pdf(data_directory):\n",
    "    loader = DirectoryLoader(data_directory, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chunks: 7486\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into chunks for better embedding\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of chunks:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face Embeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pinecone vector store from the text embeddings\n",
    "docsearch = LangchainPinecone.from_texts(\n",
    "    [t.page_content for t in text_chunks],\n",
    "    embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(metadata={}, page_content='Ellen S. Weber, MSN\\nBreast cancer\\nDefinition\\nBreast cancer is caused by the development of\\nmalignant cells in the breast. The malignant cells origi-nate in the lining of the milk glands or ducts of the breast(ductal epithelium), defining this malignancy as a cancer.Cancer cells are characterized by uncontrolled divisionleading to abnormal growth and the ability of these cellsto invade normal tissue locally or to spread throughoutthe body, in a process called metastasis.\\nDescription'), Document(metadata={}, page_content='Ellen S. Weber, MSN\\nBreast cancer\\nDefinition\\nBreast cancer is caused by the development of\\nmalignant cells in the breast. The malignant cells origi-nate in the lining of the milk glands or ducts of the breast(ductal epithelium), defining this malignancy as a cancer.Cancer cells are characterized by uncontrolled divisionleading to abnormal growth and the ability of these cellsto invade normal tissue locally or to spread throughoutthe body, in a process called metastasis.\\nDescription'), Document(metadata={}, page_content='cancerous cells. Cancer is commonly defined as the\\nuncontrolled growth of cells, with loss of differentiationand commonly, with metastasis, spread of the cancer toother tissues and organs. Cancers are malignant growths.In contrast, benign growths remain encapsulated and growwithin a well-defined area. Although benign tumors maybe fatal if untreated, due to pressure on essential organs, asin the case of a benign brain tumor , surgery or radiation')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Cancer?\"\n",
    "docs = docsearch.similarity_search(query, k=3)\n",
    "\n",
    "# Print the results\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53f375aa-3693-47f8-923e-ab4bb4eab33d\n",
      "Length of chunks: 7486\n",
      "Query result length: 384\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseRetrievalQA.from_chain_type() missing 1 required positional argument: 'llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery result length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(query_result))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# You can now run retrieval-based queries using Langchain's RetrievalQA with your Pinecone vector store\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocsearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You can replace this with your preferred chain type\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Example of using the QA chain\u001b[39;00m\n\u001b[1;32m     73\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the content of the PDF?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseRetrievalQA.from_chain_type() missing 1 required positional argument: 'llm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Set your Pinecone API key\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "print(os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"mbot\"\n",
    "\n",
    "# Initialize Pinecone connection\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Check if the index exists; if not, create it\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Adjust based on the embedding model used\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Extract data from PDFs\n",
    "def load_pdf(data_directory):\n",
    "    loader = DirectoryLoader(data_directory, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"Data/\")\n",
    "\n",
    "# Split the documents into chunks for better embedding\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of chunks:\", len(text_chunks))\n",
    "\n",
    "# Load Hugging Face Embeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "# Create a Pinecone vector store from the text embeddings\n",
    "docsearch = LangchainPinecone.from_texts(\n",
    "    [t.page_content for t in text_chunks],\n",
    "    embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query_result = embeddings.embed_query(\"hello\")\n",
    "print(\"Query result length:\", len(query_result))\n",
    "\n",
    "# You can now run retrieval-based queries using Langchain's RetrievalQA with your Pinecone vector store\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    chain_type=\"stuff\"  # You can replace this with your preferred chain type\n",
    ")\n",
    "\n",
    "# Example of using the QA chain\n",
    "query = \"What is the content of the PDF?\"\n",
    "result = qa_chain.run(query)\n",
    "print(\"Query result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53f375aa-3693-47f8-923e-ab4bb4eab33d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "print(os.getenv(\"PINECONE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
